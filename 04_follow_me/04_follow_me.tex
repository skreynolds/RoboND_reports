\documentclass[a4paper]{article}

%--------------------------------------------------------------------------

\usepackage[a4paper, total={6in, 9in}]{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\usepackage{inconsolata}
\usepackage{listings}
\usepackage{pstricks-add}
\usepackage{siunitx}
\usepackage[most]{tcolorbox}
\usepackage{tikz}
\usepackage{epstopdf} %converting to PDF
\usepackage{hyperref}
\usepackage{makecell}
\usepackage{siunitx}

\usetikzlibrary{shapes.geometric}

%------------------------------------------------------------
\graphicspath{{./fig/}}

%------------------------------------------------------------
\setlength{\parindent}{0in}

\lstdefinestyle{Python}{
	language        = Python,
	basicstyle      = \ttfamily,
	keywordstyle    = \color{blue},
	keywordstyle    = [2] \color{teal}, % just to check that it works
	stringstyle     = \color{green},
	commentstyle    = \color{red}\ttfamily
}

%------------------------------------------------------------
\newtcblisting[auto counter]{sexylisting}[2][]{sharp corners, 
    fonttitle=\bfseries, colframe=gray, listing only, 
    listing options={basicstyle=\ttfamily,language=Python}, 
    title=Listing \thetcbcounter: #2, #1}

%--------------------------------------------------------------------------

\begin{document}
\title{Udacity: Follow Me Report}
\author{Shane Reynolds}
\maketitle
\tableofcontents

\newpage

\section{Introduction}
Computer vision is a subset of robotic perception - it has been defined as the development of autonomous systems which can perform tasks achieved by human visual systems (Huang, 1996). This means the acquisition of digital image data from an optical camera, and some type of interpretation of the acquired image. A simple example of a task that is routinely performed by a human visual system, which is sought for computer vision systems, is answering the question: \textit{Is there a puppy in Figure 1?}, or \textit{Where is the puppy in Figure 1?}.
\begin{figure}[h]
\centering
\frame{\includegraphics[scale=0.15]{puppy}}
\caption{Computer vision is interested in answering questions such as \textit{Is there a puppy in the image?} or \textit{Where is the puppy in the image?}}
\end{figure}

There are many sub-fields of computer vision such as scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, and motion estimation. This paper will focus on classification, using an approach called semantic segmentation. Shelhamer, Long and Darrell (2016) define semantic segmentation as a method of inference which is able to categorise fine image details. This is achieved by classifying each pixel in the image, and labelling it with the class of its enclosing object or region. A Fully Convolutional Neural Network (FCN) is proposed as the architecture to implement semantic segmentation. An FCN model was trained and implemented on a robotic agent.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\centering
\frame{\includegraphics[height=6cm]{agent_image}}
\caption{The UAV agent in the simulated environment.}
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.45\textwidth}
\centering
\frame{\includegraphics[height=6cm]{hero_image}}
\caption{The \textit{hero} can be seen coloured in red, with the UAV agent following her.}
\end{minipage}
\end{figure}

The agent was tasked with the identification (and subsequent tracking) of an individual, known as the \textit{hero}, in a 3D simulated environment built with Unity. The 3D simulated environment is a small city consisting of buildings, roads, elevated highways, and vegetation. The robotic agent is an unmanned aerial vehicle (UAV), as shown in Figure 2, which is fitted with a panning optical camera. The agent roams the simulation until it is able to locate the hero using the trained FCN, at which point the UAV will track the hero. The hero is a simulated person, as seen in Figure 3. To satisfy the assignment criteria, the proposed FCN model must achieve above a 40\% benchmark for an intersection over union metric. 

\section{Network Architecture}
FCNs are widely used for computer vision applications, and are a type of Artificial Neural Network (ANN). ANNs are computational models which, once trained on a dataset, can be used to make classification predictions, or  value estimations, based on a set of feature inputs that the model has been trained on. A typical fully connected feed-forward ANN consists of an input layer, one or more hidden layers, and an output layer, as shown in Figure 4. Hidden layers are made up of multiple nodes. The nodes themselves contain a non-linear activation function, such as a sigmoid or ReLU, and receive weighted input from the previous layers in the model. The inputs from the previous layer, and the non-linear activation of a node form a computational element called a neuron (also known as a perceptron) - these can be loosely thought of as decision making elements. An example of a neuron can seen in Figure 5.
\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[scale=0.25]{feed_forward_neural_net}
\caption{A feed-forward artificial neural network consists of an input layer, which receives feature inputs, some hidden layers, and an output layer for classification.}
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[scale=0.55]{perceptron}
\caption{The structure of a neuron (perceptron) includes weighted inputs from the previous layer, and a non-linear activation function.}
\end{minipage}
\end{figure}

Changing the weights in a neuron changes the neurons's contribution to the model, which in turn affects the overall model output. Weight changes occur during model training, which uses large volumes of labelled data to adjust the weights. Hidden layers are important because they allow highly non-linear models to be constructed, providing an approach for estimating complex phenomena which may be difficult to model with classical approaches, or computationally intractable. Generally, the more hidden layers, the more non-linear the model. Network architectures with multiple hidden layers have become so wide spread that the term Deep Neural Network (DNN) was coined to describe feed-forward ANNs which use two or more hidden layers. It must be noted that whilst increased non-linearity may allow us to model more complex phenomenon, making the ANN deeper does not guarantee increased model performance. This is mainly due to the fact that deeper models may over-fit the data during training, resulting in a failure to generalise on test and validation data sets.  

\begin{table}[h]
\centering
\caption{Reproduced from Ciresan, Meier, Gambardella, and Schmidhuber (2010) - DNN architectures of varying size for classifying the MNIST data set, and the associated performance of each network.} \footnotesize
\begin{tabular}{lS[table-format=3.2]S[table-format=3.2]S[table-format=3.2]S[table-format=3.2]}
\toprule
{\textbf{Architecture}} & {\textbf{Test Error}} & {\textbf{Best Test}} & {\textbf{Simulation}} & {\textbf{Weights}}\\
{\textbf{(number of neurons in each layer)}} & {\textbf{Best Validation [\%]}} & {\textbf{Error [\%]}} & {\textbf{Time [min]}} & {\textbf{[Millions]}}\\
\midrule
1000, 500, 10 & 0.49 & 0.44 & 23.4 & 1.34\\
1500, 1000, 500, 10 & 0.46 & 0.40 & 44.2 & 3.26\\
2000, 1500, 1000, 500, 10 & 0.41 & 0.39 & 66.7 & 6.69\\
2500, 2000, 1500, 1000, 500, 10 & 0.35 & 0.32 & 114.5 & 12.11\\
9 $\times$ 1000, 10 & 0.44 & 0.43 & 107.7 & 8.86\\
\bottomrule
\end{tabular}
\end{table}

Fully connected feed-forward DNNs have proven effective in computer vision classification problems, such as optical character recognition. One of the most widely cited examples of this is a feed-forward DNN performing classification on the MNIST dataset. The MNIST dataset contains handwritten digits, from 0 to 9, and is considered a benchmark for measuring neural net classification performance for the optical character recognition problem. Table 1, taken from paper by Ciresan, Meier, Gambardella, and Schmidhuber (2010), shows a table of feed-forward DNNs with varying numbers of hidden layers, and hidden layer depth.

\vspace{0.5cm}

\begin{minipage}{0.45\textwidth}
Notably, every model listed presents an error rate of less than 1\%. Generally, the deeper a network, the better the model's predictive performance, although this is not always the case as previously outlined. Figure 5, taken from the same paper, shows a small set of the misclassified images. Despite the misclassification it can be seen that, in most cases, the handwritten digit bears a high resemblance to the predicted value, and that the second prediction is generally correct. Remarkably, DNNs are not considered state of the art for image classification problems - even with simple tasks like MNIST classification. This is due to model inefficiencies that arise from image variation in the spatial domain.
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[height=3.5cm]{misclassification}
\captionof{figure}{Misclassified hand written digits by the top performing DNN from Ciresan, Meier, Gambardella, and Schmidhuber (2010). The digit in the top right hand corner of the box is the observation label, and the two digits in the bottom right hand corner are the predictions from the DNN model.}
\end{minipage}

\vspace{0.5cm}

The problem can be better understood by considering Figures 7 and 8. Figure 7 shows an image with a puppy on the left, and Figure shows an image with a puppy on the right. Suppose we create a simple model to classify whether an image has a puppy in it or not, and assume we train this model with lots of images like the one shown in Figure 7. If the trained model was then used to classify images like the one shown in Figure 8 it would perform poorly. This is because our model would have only learned to classify pixel features on the left side of the picture with puppies, which says nothing about identifying a puppy on the right hand side of an image. Put simply, there is no \textit{translational invariance} in the model.

\vspace{0.25cm}

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\centering
\frame{\includegraphics[height=4cm]{doggo_left}}
\caption{An image in which a puppy is located on the left hand side of the image.}
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.45\textwidth}
\centering
\frame{\includegraphics[height=4cm]{doggo_right}}
\caption{An image in which a puppy is located on the right hand side of the image.}
\end{minipage}
\end{figure}

\newpage

\subsection{Convolutional Neural Networks}
A Convolutional Neural Network (CNN) is a class of ANN, which has an underlying network structure which is better at learning shapes, edges, and colours meaning it is less reliant on the spatial location of a classification object in an image. Recall that vanilla feed-forward neural nets only have neuron connections from the previous layer, and there are no connections from neurons in the same layer - weights are not shared. In contrast, CNNs share neuron weights by using filters which are convolved over an input image. Consider a raw input image of say 32 $\times$ 32 pixels, with a depth of 3 colour channels, as shown in Figure 9.
\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[scale=0.4]{CNN}
\caption{The raw image input is 32 $\times$ 32, with a 3-channel depth (R,G,B). The filter is a 3 $\times$ 3 patch with the same depth as the input. The filter is convolved over the image using some stride - each convolution creates a single element output which forms part of the 2D activation map (i.e. the output). There are $K$ filters convolved over the image, a parameter chosen as part of the architecture, and the output volume represents the stacked 2D activation maps.}
\vspace{0.5cm}
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[scale=0.43]{CNN_example}
\caption{An simple example of weight sharing that takes place in a CNN. The white 1D array represents the input image, and the blue 1D array represents the filter. The filter (blue) is convolved across the input (white) using a stride of 2. The convolved output, which represents the activation map, can be seen in yellow. This architecture allows for the sharing of the weights in the model.}
\end{minipage}
\end{figure}

The convolving filter, which contains the model weights, is the small patch which is incident on the raw input image surface. The image section in contact with the filter is called the receptive field - this changes as the filter convolves an image. Filter width and height are parameters chosen as part of the network architecture, and filter depth is identical to the input image depth. The filter is moved around the image according to the number of pixels in each stride. After each movement, the filter weights are multiplied by the receptive field and added together - this makes up a single entry in the 2D activation map which forms part of the output volume. The width and height of the output volume are dependent on the stride, and the type of padding used during the convolution. Finally, the output depth is dependent on the number of filters specified in the network architecture - typically this parameter is denoted as $K$. The output volume is simply the stacked 2D activation maps from each filter. It is the convolutional process, whereby the filter weights are used over an entire image, that provide the weight sharing seen in CNNs. Figure 10 provides a simple example of how this works. In this example, the filter is a 1D array (shown in blue), and the input image is also a 1D array (shown in white). The filter (blue) is convolved over the input image (white) with a stride of 2. The filter weights are multiplied with the image values, and added together to form the output volume - a 1D array (shown in yellow).\\

CNNs provide superior performance over DNNs in the image classification domain. This was demonstrated on the MNIST dataset by LeCun (XXXX) with his LeNet (pictured in Figure 11), and again by Krizhevsky (XXXX) on the ImageNet dataset with AlexNet (shown in Figure 12). Whilst CNNs can achieve noteworthy performance in the task of image classification, there is a notable performance loss when they are re-tasked with pixel by pixel classification known as semantic segmentation. This is due to the fact that fully connected layers don't preserve spatial information.
\begin{figure}[h]
\centering
\includegraphics[scale=0.45]{lenet5}
\caption{text}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{alexnet}
\caption{text}
\end{figure}

\clearpage

\subsection{Fully Convolutional Neural Networks}
FCNs preserve spatial information, and represent a state-of-the-art approach to semantic segmentation. Structurally, they can be thought of as two distinct parts: encoders and decoders. This is shown in Figure 13. The encoder is comprised of several convolutional layers, which are typically arranged to progressively concentrate the spatial domain, and increase the number of channels in the image. These different layers of spatial compression are useful for training the model on different image resolutions. The decoder, in contrast, upsamples encoder compressions, restoring the spatial information to the output - typically the final layer restores the output to the initial input image dimensions, before being passed to a convolutional layer with a softmax activation function.
\vspace{0.25cm}
\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{FCN}
\caption{Structurally FCNs can be thought of as an encoder (convolutional network) connected to a decoder (deconvolutional network)}
\end{figure}

FCNs are often make use of 1 $\times$ 1 convolutions, which are just a 2D convolution of patch size 1 $\times$ 1, and a stride of 1 (NETWORK IN NETWORK). The spatial dimensions of the convolved image are preserved in the output volume, but the desired number of filters ($K$) changes the output volume depth. Indeed this is one of the main incentives to use 1 $\times$ 1 convolutions - they are computationally cheap, and can reduce the image depth making subsequent operations less expensive. One approach that has been used is to intersperse convolutional layers with 1 $\times$ 1 convolutional layers, to reduce the depth of the image being convolved by larger patches (e.g. 3 $\times$ 3, or 5 $\times$ 5). An example of this can be seen in Figure XXXX. Of course, this is not the only way that 1 $\times$ 1 convolutional layers can be used. They can also be used to add more depth to a model, and additional non-linearity since the 1 $times$ 1 layer contains an activation function like ReLU. Using 1 $\times$ 1 convolutions is considered a cheap way of doing this.  
\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{1by1}
\caption{The input is 28 $\times$ 28 $\times$ 192 and the desired 2D convolution is a 5 $\times$ 5 patch using 32 filters. This would result in a total of XXXX multiplications. If a 1 $\times$ 1 convolution is used to reduce the depth of the input image from 192 to 16 before performing the 2D convolution with the 5$\times$5 patch, this reduces the total number of multiplications to XXXX.}
\end{figure}

Finally, FCNs see an improvement in model performance with the use of something called skip connections. A skip connection attaches a residual output in an FCN architecture to an output occurring later in the network. Their use was originally proposed by XXXX XXXXX and XXXXX (XXXXX) who found that adding more layers to their model \textit{worsened} the performance during training. This work led to the development of the well known Resnet architecture. The skip connections essentially concatenate the 2 layers by adding the outputs - an example of the operation can be seen in Figure XXXX.
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{skip}
\caption{An example of a skip connection - the residual output in the network literally skips a section of the network and is concatenated with a later output of the model.}
\end{figure}

\subsection{Proposed Architectures \& Implementation}
The information presented so far makes a compelling case for the use of FCNs to perform the semantic segmentation task, however, there is some uncertainty as to what model architecture should be employed. State of the art FCNs involve multiple 2D convolution layers interspersed with pooling layers in the encoder. The decoder consists of up-sampling layers interspersed with un-pooling layers. Additionally, these state of the art networks make use of 1 $\times$ 1 convolutions, and skip connections.An example of this can be seen in Figure XXXX, which shows an FCN network called SegNet taken from a paper by XXXX, XXXX, and XXXX (XXXX).
\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{segnet}
\caption{text}
\end{figure}

FCN networks with many layers, like the original FCN architecture by XXXX, XXXX, and XXXX (XXXX) and SegNet, are used to provide semantic segmentation on images with high visual complexity taken from the real world. In contrast, the models proposed in this paper are tasked with semantic segmentation of images taken from the 3D environment simulated in Unity. Given this, it may not be appropriate to adopt the depth seen in state-of-the-art FCN architectures since reasonable performance may be achieved with smaller models. Moreover, given that the dataset from the simulation only consists of approximately 4000 images (about half of the 9,993 found in the VOC2012 segmentation data set), a more complex model may result in worse performance.

% The main issue with this is that these networks were built for real world semantice segmentation with lots and lots of data and lots and lots of compute - the problem that we are dealing with does not have this level of complexity in the image data, nor does it have a comparable dataset or compute for training - so we have to make sure that the model is orders of magnitude simpler.

% The benchmark that needs to be satisfied - how does this work?

% With this in mind propose the following models for experimentation...

Three different FCN architectures are proposed for investigation to satisfy the 40\% intersection over union benchmark assignment criteria: a deep model with 1 $\times$ 1 convolutions; a shallow model with 1 $\times$ 1 convolutions; and a shallow model without 1 $\times$ 1 convolutions. These models are discussed in more detail in Sections 2.3.2, 2.3.3, and 2.3.4 respectively.

\subsubsection{Tensorflow Implementation}
To make the model implementation easier, two functions were developed: \verb|encoder_block()| (shown in Listing 1), and \verb|decoder_block()| (shown in Listing 2). An interesting feature of the \verb|encoder_block()| is that it uses the a seperable 2D convolution. The reason for this is that separable convolutions are more computationally efficient. To see this, consider a 3 $\times$ 3 patch with one filter ($K = 1$) in a single convolution. The number of operations to perform this is XXXX PROVIDE EXPLANATION. A matrix is seperable if it can be written as follows:
\begin{align}
\begin{bmatrix}
A \cdot a & A \cdot b & A \cdot c\\
B \cdot a & B \cdot b & B \cdot c\\
C \cdot a & C \cdot b & C \cdot c
\end{bmatrix}
=
\begin{bmatrix}
A\\
B\\
C
\end{bmatrix}
\cdot
\begin{bmatrix}
a & b & c
\end{bmatrix}
\end{align}  

The definition shown in (1) can be used to first undertake a convolution with XXXX, and then convolve the result of the first convolution with XXXX. This results in a total of XXXX operations. Given that FCNs have to perform this type of operation many times to convolve an entire image, there are significant computational savings made by using separable 2D convolutions.
   
\begin{figure}[h]\scriptsize
\begin{sexylisting}{text}
def encoder_block(input_layer, filters, strides):
    
    # TODO Create a separable convolution layer using the separable_conv2d_batchnorm() function.
    output_layer = separable_conv2d_batchnorm(input_layer, filters, strides)
    
    return output_layer
\end{sexylisting}
\end{figure}

The \verb|decoder_block()| function takes three arguements, two of them being layers. The smallest input layer is upsampled using the \verb|bilinear_upsample()| function. WHAT DOES THIS DO?? The second layer input is a skip connection which comes from the encoder section of the FCN. The upsampled layer and the layer from the skip connection are added together using a concatenate function. Note that care must be taken to ensure there is no mismatch in the layer dimensions. The concatenated layers are then passed through a 2D convolution to create the final layer.  

\begin{figure}[h]\scriptsize
\begin{sexylisting}{text}
def decoder_block(small_ip_layer, large_ip_layer, filters):
    
    # TODO Upsample the small input layer using the bilinear_upsample() function.
    small_ip_upsample = bilinear_upsample(small_ip_layer)
    
    # TODO Concatenate the upsampled and large input layers using layers.concatenate
    concat_layer = layers.concatenate([small_ip_upsample, large_ip_layer])
    
    # TODO Add some number of separable convolution layers
    output_layer = separable_conv2d_batchnorm(concat_layer, filters)
    
    return output_layer
\end{sexylisting}
\end{figure}

Both the \verb|encoder_block()| and \verb|decoder_block()| functions use ReLUs as the non-linear activation functions, and both functions employ batch normalisation. Batch normalisation in this instance WHAT DOES THE BATCH NORMALISATION DO??

\clearpage

\subsubsection{Model 1: Deep Model with 1 $\times$ 1 Convolution}
The proposed deep model with 1 $\times$ 1 convolutions is comprised of 9 layers, including the input layer - the network architecture can be seen in Figure XXXX, and a detailed description of each layer can be found in Table XXXX
\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.83, every node/.style={scale=0.83}]
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
            
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[scale=0.33]{model_3(1_paper)}};
    
    \node[] at (1.8,1) {\circled{1}};
    \node[] at (3.8,0.718) {\circled{2}};
    \node[] at (5.8,0.437) {\circled{3}};
    \node[] at (8,0.1291) {\circled{4}};
    \node[] at (10.2,-0.1795) {\circled{5}};
    \node[] at (12,-0.4322) {\circled{6}};
    \node[] at (13.2,-0.6) {\circled{7}};
    
    \draw[->, >=stealth] (4,4) to [out=20,in=120] (10.2,3.2);
    \draw[->, >=stealth] (2.2,4.5) to [out=20,in=120] (12,3.1);
    \draw[->, >=stealth] (1,4.8) to [out=20,in=120] (13.8,3);
\end{tikzpicture}
\caption{text}
\end{figure}

\begin{table}[h]
\centering
\caption{text}\scriptsize
\begin{tabular}{ccccp{8cm}}
\toprule
\textbf{Name} & \textbf{Width (W)} & \textbf{Height (W)} & \textbf{Depth (D)} & Layer Details\\  
\midrule
 & & & &\\
Input & 160 & 160 & 3 & Raw image input\\
 & & & &\\
1 & 80 & 80 & 32 & Created from 2D convolution of raw image input with 32 filters of patch size 3 $\times $ 3, and the stride is 2 with same padding\\
 & & & &\\
2 & 40 & 40 & 64 & Created from 2D convolution of layer 1 with 64 filters of patch size 3 $\times $ 3, and the stride is 2 with same padding\\
 & & & &\\
3 & 20 & 20 & 128 & Created from 2D convolution of layer 2 with 128 filters of patch size 3 $\times$ 3, and the stride is 2 with same padding\\
 & & & &\\
4 & 20 & 20 & 128 & Created from 2D convolution of layer 3 with 128 filters of patch size 1 $\times$ 1, and the stride is 1 with same padding\\
 & & & &\\
5 & 40 & 40 & 64 & Created by adding upsampled layer 4 to layer 2 with a skip connection, and passed through a 2D convolution with 64 filters of patch size 3 $\times$ 3 and stride XXXX\\
 & & & &\\
6 & 80 & 80 & 32 & Created by adding upsampled layer 5 to layer 1 with a skip connection, and passed through a 2D convolution with 32 filters of patch size 3 $\times$ 3 and stride XXXX\\
 & & & &\\
7 & 160 & 160 & 3 & Created by adding upsampled layer 6 to the inputs with a skip connection, and passed through a 2D convolution with 3 filters of patch size 3 $\times$ 3 and stride XXXX\\
& & & &\\
Output & 160 & 160 & 3 & Created from a 2D convolution\\
\bottomrule
\end{tabular}
\end{table}

\newpage

\subsubsection{Model 2: Shallow Model with 1 $\times$ 1 Convolution}
The proposed deep model with 1 $\times$ 1 convolutions is comprised of 9 layers, including the input layer - the network architecture can be seen in Figure XXXX, and a detailed description of each layer can be found in Table XXXX 
\begin{figure}[h]
\centering
\begin{tikzpicture}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
            
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[scale=0.33]{model_2(2_paper)}};
    
        \node[] at (2.3,1) {\circled{1}};
        \node[] at (4.8,0.7) {\circled{2}};
        \node[] at (7.3,0.4) {\circled{3}};
        \node[] at (9.8,0.1) {\circled{4}};
        \node[] at (12.3,-0.2) {\circled{5}};
	    
	    \draw[->, >=stealth] (2.8,4.5) to [out=20,in=120] (10.5,3.8);
	    \draw[->, >=stealth] (1.3,4.9) to [out=20,in=120] (13,3.6);
	    
\end{tikzpicture}
\caption{text}
\end{figure}

\begin{table}[h]
\centering
\caption{text}\scriptsize
\begin{tabular}{ccccp{8cm}}
\toprule
\textbf{Name} & \textbf{Width (W)} & \textbf{Height (W)} & \textbf{Depth (D)} & Layer Details\\  
\midrule
 & & & &\\
Input & 160 & 160 & 3 & Raw image input\\
 & & & &\\
1 & 80 & 80 & 32 & Created from 2D convolution of raw image input with 32 filters of patch size 3 $\times $ 3, and the stride is 2 with same padding\\
 & & & &\\
2 & 40 & 40 & 64 & Created from 2D convolution of layer 1 with 64 filters of patch size 3 $\times $ 3, and the stride is 2 with same padding\\
 & & & &\\
3 & 40 & 40 & 128 & Created from 2D convolution of layer 2 with 128 filters of patch size 1 $\times$ 3, and the stride is 1 with same padding\\
 & & & &\\
4 & 80 & 80 & 64 & Created by adding upsampled layer 3 to layer 1 with a skip connection, and passed through a 2D convolution with 64 filters of patch size 3 $\times$ 3\\
 & & & &\\
5 & 160 & 160 & 32 & Created by adding upsampled layer 4 to the raw input with a skip connection, and passed through a 2D convolution with 32 filters of patch size 3 $\times$ 3\\
 & & & &\\
Output & 160 & 160 & 3 & Created from a 2D convolution\\
\bottomrule
\end{tabular}
\end{table}

\newpage

\subsubsection{Model 3: Shallow Model without 1 $\times$ 1 Convolution}
The proposed deep model with 1 $\times$ 1 convolutions is comprised of 8 layers, not including the input layer - the network architecture can be seen in Figure XXXX, and a detailed description of each layer can be found in Table XXXX 
\begin{figure}[h]
\centering
\begin{tikzpicture}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
            
    \node[anchor=south west,inner sep=0] (image) at (0,0) 			{\includegraphics[scale=0.33]{model_1(3_paper)}};

    \node[] at (2.5,1) {\circled{1}};
    \node[] at (4.8,0.7183) {\circled{2}};
    \node[] at (7.3,0.4122) {\circled{3}};
    \node[] at (9.8,0.1061) {\circled{4}};
    \node[] at (12.3,-0.199) {\circled{5}};
    
    \draw[->, >=stealth] (3,4.3) to [out=20,in=120] (10.5,3.5);
	\draw[->, >=stealth] (1.3,4.7) to [out=20,in=120] (13,3.4);
\end{tikzpicture}
\caption{text}
\end{figure}

\begin{table}[h]
\centering
\caption{text}\scriptsize
\begin{tabular}{ccccp{8cm}}
\toprule
\textbf{Name} & \textbf{Width (W)} & \textbf{Height (W)} & \textbf{Depth (D)} & Layer Details\\  
\midrule
 & & & &\\
Input & 160 & 160 & 3 & Raw image input\\
 & & & &\\
1 & 80 & 80 & 32 & Created from 2D convolution of raw image input with 32 filters of patch size 3 $\times $ 3, and the stride is 2 with same padding\\
 & & & &\\
2 & 40 & 40 & 64 & Created from 2D convolution of layer 1 with 64 filters of patch size 3 $\times $ 3, and the stride is 2 with same padding\\
 & & & &\\
3 & 40 & 40 & 64 & Created from 2D convolution of layer 2 with 64 filters of patch size 3 $\times$ 3, and the stride is 1 with same padding\\
 & & & &\\
4 & 80 & 80 & 64 & Created by adding upsampled layer 3 to layer 1 with a skip connection, and passed through a 2D convolution with 64 filters of patch size 3 $\times$ 3\\
 & & & &\\
5 & 160 & 160 & 32 & Created by adding upsampled layer 4 to raw inputs with a skip connection, and passed through a 2D convolution with 32 filters of patch size 3 $\times$ 3\\
& & & &\\
Output & 160 & 160 & 3 & Created from a 2D convolution\\
\bottomrule
\end{tabular}
\end{table}

\clearpage

\section{Network Training}
The initial data set provided with the problem was comprised of XXXX images. There are three rough categories that the images can be placed in:
\begin{enumerate}
\item 
\item 
\item 
\end{enumerate}
The network is trained with Stochastic Gradient Descent - specifically using the Adam optimizer. How does this optimiser work?

Talk about the use of hyperparameters for training the network

%Talk about the pre-processing of the data set (what does this do?)

%Talk about the 

\subsection{Batch Size}
\begin{minipage}{0.45\textwidth}
The batch size defines the number of 
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[scale=0.5]{hyp_par_1}
\captionof{figure}{text}
\end{minipage}

\subsection{Epochs}
\begin{minipage}{0.45\textwidth}
Epochs are 
\end{minipage}
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[scale=0.5]{hyp_par_2}
\captionof{figure}{text}
\end{minipage}


\subsection{Learning Rate}
\begin{minipage}{0.45\textwidth}
Talk about the learning rate being critical to 
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[scale=0.5]{hyp_par_3}
\captionof{figure}{text}
\end{minipage}


\section{Performance \& Model Generalisation}

\section{Future Enhancements}
% Collecting addtional datasets and filtering them to only include images of the hero - note that we want data that does not include the 
\end{document}